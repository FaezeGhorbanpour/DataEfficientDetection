{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:12.883968Z",
     "start_time": "2024-12-21T18:51:12.876367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mounts/work/faeze/.cache/hf/'\n",
    "os.environ['HF_HOME'] = '/mounts/work/faeze/.cache/hf/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mounts/work/faeze/.cache/hf/'\n",
    "os.environ['TORCH_HUB'] = '/mounts/work/faeze/.cache/torch/'\n",
    "os.environ['TORCH_HOME'] = '/mounts/work/faeze/.cache/torch/'\n",
    "os.environ[\"WANDB_DIR\"] = '/mounts/work/faeze/.cache/wandb/'"
   ],
   "id": "dc6f27063f71fb1b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:17.655294Z",
     "start_time": "2024-12-21T18:51:12.956893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_provider import DataProvider\n",
    "from finetuner import FineTuner, FineTunerConfig"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:17.897338Z",
     "start_time": "2024-12-21T18:51:17.893894Z"
    }
   },
   "cell_type": "code",
   "source": "data_provider_ = DataProvider()",
   "id": "3ba5e89364e192c2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:18.013057Z",
     "start_time": "2024-12-21T18:51:18.001570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "names = ['bas19_es', 'dyn21_en', 'for19_pt', 'fou18_en', 'has21_hi', 'ken20_en',\n",
    "                       'ous19_ar', 'ous19_fr', 'san20_it',]\n",
    "\n",
    "datas = [f'baseline_data/{i}' for i in names]\n",
    "languages = [name.split('_')[1] for name in names]\n",
    "languages"
   ],
   "id": "4d42bd732f96c9e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es', 'en', 'pt', 'en', 'hi', 'en', 'ar', 'fr', 'it']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:18.549384Z",
     "start_time": "2024-12-21T18:51:18.148616Z"
    }
   },
   "cell_type": "code",
   "source": "datasets = data_provider_.load_datasets(datas, languages)",
   "id": "f898dccaf54679a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/faeze/new_miniconda3/envs/2024_9/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for baseline_data contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at baseline_data/baseline_data.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:51:18.595369Z",
     "start_time": "2024-12-21T18:51:18.588232Z"
    }
   },
   "cell_type": "code",
   "source": "datasets[5]",
   "id": "5b1fb19fa2855706",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'baseline_data/ken20_en',\n",
       " 'split': 'test',\n",
       " 'data': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'text', 'label'],\n",
       "         num_rows: 20692\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'text', 'label'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['id', 'text', 'label'],\n",
       "         num_rows: 2000\n",
       "     })\n",
       " }),\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:20:33.125753Z",
     "start_time": "2024-12-21T21:20:33.120429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = datasets[0]['data']['train']\n",
    "eval_data = datasets[0]['data']['validation']\n",
    "test_data = datasets[0]['data']['test']"
   ],
   "id": "e1aa8daf76648217",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:20:42.911261Z",
     "start_time": "2024-12-21T21:20:42.902045Z"
    }
   },
   "cell_type": "code",
   "source": "datasets",
   "id": "b081d7fb2f87ccbe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'baseline_data/bas19_es',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 4100\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'es'},\n",
       " {'name': 'baseline_data/dyn21_en',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 38644\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'en'},\n",
       " {'name': 'baseline_data/for19_pt',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 3170\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'pt'},\n",
       " {'name': 'baseline_data/fou18_en',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 20065\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'en'},\n",
       " {'name': 'baseline_data/has21_hi',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2094\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'hi'},\n",
       " {'name': 'baseline_data/ken20_en',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 20692\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'en'},\n",
       " {'name': 'baseline_data/ous19_ar',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2053\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 300\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 1000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'ar'},\n",
       " {'name': 'baseline_data/ous19_fr',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2014\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 1500\n",
       "      })\n",
       "  }),\n",
       "  'language': 'fr'},\n",
       " {'name': 'baseline_data/san20_it',\n",
       "  'split': 'test',\n",
       "  'data': DatasetDict({\n",
       "      train: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 5600\n",
       "      })\n",
       "      validation: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 500\n",
       "      })\n",
       "      test: Dataset({\n",
       "          features: ['id', 'text', 'label'],\n",
       "          num_rows: 2000\n",
       "      })\n",
       "  }),\n",
       "  'language': 'it'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:54:38.189287Z",
     "start_time": "2024-12-21T18:54:38.186Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "27f93fd294af98c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:54:38.878528Z",
     "start_time": "2024-12-21T18:54:38.873757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "config = FineTunerConfig(\n",
    "        model_name=\"FacebookAI/xlm-roberta-base\",\n",
    "        fine_tune_method=\"default\",\n",
    "        num_labels=2,  # Binary classification (Hate Speech detection)\n",
    "        learning_rate=5e-5,\n",
    "        epochs=5,\n",
    "        batch_size=16,\n",
    "    # peft_config={\"lora_rank\": 4}\n",
    "    )"
   ],
   "id": "a8477676d4da1438",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:38:42.371458Z",
     "start_time": "2024-12-19T14:38:40.545449Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuner = FineTuner(config)",
   "id": "576eefc2f5830ab2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:38:43.177890Z",
     "start_time": "2024-12-19T14:38:42.422063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = fine_tuner.prepare_data(train_data)\n",
    "eval_dataset = fine_tuner.prepare_data(eval_data)\n",
    "test_dataset = fine_tuner.prepare_data(test_data)"
   ],
   "id": "f19ac87f14300225",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:46:49.341183Z",
     "start_time": "2024-12-19T14:38:43.965321Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuner.train(train_dataset, eval_dataset)",
   "id": "fe7daa108014765e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 08:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669700</td>\n",
       "      <td>0.565356</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.671097</td>\n",
       "      <td>0.691927</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.691792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.817341</td>\n",
       "      <td>0.837512</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.829477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.434600</td>\n",
       "      <td>0.387590</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.815761</td>\n",
       "      <td>0.830115</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.827029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.426065</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.821995</td>\n",
       "      <td>0.837824</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.827869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.384766</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.844757</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.852134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:37:32.319676Z",
     "start_time": "2024-12-19T14:36:51.626948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = fine_tuner.predict(test_dataset)\n",
    "results = fine_tuner.compute_metrics(prediction)\n",
    "print(results)"
   ],
   "id": "71ef10423348daff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/faeze/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8015, 'f1-macro': 0.7990011318210826, 'precision': 0.8074411137805951, 'recall': 0.8015, 'f1-weighted': 0.8025869516293944}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:37:32.678363Z",
     "start_time": "2024-12-19T14:37:32.459352Z"
    }
   },
   "cell_type": "code",
   "source": "{'accuracy': 0.782, 'f1-macro': 0.7790265816831281, 'precision': 0.7872806227483272, 'recall': 0.782, 'f1-weighted': 0.7831278483270894}",
   "id": "dbbeaffac57317c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.782,\n",
       " 'f1-macro': 0.7790265816831281,\n",
       " 'precision': 0.7872806227483272,\n",
       " 'recall': 0.782,\n",
       " 'f1-weighted': 0.7831278483270894}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:37:33.026065Z",
     "start_time": "2024-12-19T14:37:32.800730Z"
    }
   },
   "cell_type": "code",
   "source": "{'accuracy': 0.758, 'f1-macro': 0.715371469635899, 'precision': 0.7543940303940304, 'recall': 0.758, 'f1-weighted': 0.7559071264162328}",
   "id": "cd2ab993a4db9d54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.758,\n",
       " 'f1-macro': 0.715371469635899,\n",
       " 'precision': 0.7543940303940304,\n",
       " 'recall': 0.758,\n",
       " 'f1-weighted': 0.7559071264162328}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
