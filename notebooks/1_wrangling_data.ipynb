{
 "cells": [
  {
   "cell_type": "code",
   "id": "f13adda2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:23.526172Z",
     "start_time": "2025-01-20T18:42:23.521697Z"
    }
   },
   "source": [
    "# import relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "id": "f2dbcb7d-80d9-42c9-b391-cf6e785ae5eb",
   "metadata": {},
   "source": [
    "## Load raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "751cc58c-ca94-42c7-b1bd-a1d1e23ebdd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:23.969553Z",
     "start_time": "2025-01-20T18:42:23.916982Z"
    }
   },
   "source": [
    "df_dict = dict()\n",
    "\n",
    "PATH = \"/mounts/data/proj/faeze/data_efficient_hate/datasets/main/\"\n",
    "\n",
    "for file in sorted(os.listdir(PATH+'0_raw')):\n",
    "    if 'measure' in file:\n",
    "        continue\n",
    "    if 'gahd' not in file:# and 'xdomain' not in file and 'implicit' not in file and 'xplain' not in file:\n",
    "        continue\n",
    "    if \"ipynb\" not in file:\n",
    "        if file.endswith(\".csv\"):\n",
    "            print(re.sub('\\.csv$', '', file))\n",
    "            df_dict[re.sub('\\.csv$', '', file)] = pd.read_csv(f\"{PATH}/0_raw/{file}\", on_bad_lines='skip')\n",
    "        else:\n",
    "            print(re.sub('\\.tsv$', '', file))\n",
    "            df_dict[re.sub('\\.tsv$', '', file)] = pd.read_csv(f\"{PATH}/0_raw/{file}\", on_bad_lines='skip', delimiter='\\t')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gahd24_de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_170674/2774132277.py:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  print(re.sub('\\.csv$', '', file))\n",
      "/tmp/ipykernel_170674/2774132277.py:13: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  df_dict[re.sub('\\.csv$', '', file)] = pd.read_csv(f\"{PATH}/0_raw/{file}\", on_bad_lines='skip')\n",
      "/tmp/ipykernel_170674/2774132277.py:15: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  print(re.sub('\\.tsv$', '', file))\n",
      "/tmp/ipykernel_170674/2774132277.py:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  df_dict[re.sub('\\.tsv$', '', file)] = pd.read_csv(f\"{PATH}/0_raw/{file}\", on_bad_lines='skip', delimiter='\\t')\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:24.400156Z",
     "start_time": "2025-01-20T18:42:24.380854Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict[\"implicit_en\"]['label'].value_counts()",
   "id": "33d6cea71c5e08b7",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'implicit_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimplicit_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts()\n",
      "\u001B[0;31mKeyError\u001B[0m: 'implicit_en'"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "id": "bed7ed05-5188-48b5-a2ca-338ebadd806b",
   "metadata": {},
   "source": [
    "## Reformat columns\n",
    "Need separate logic for different datasets. 1 is for hateful, 0 for non-hateful."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:26.830596Z",
     "start_time": "2025-01-20T18:42:26.810170Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict[\"implicit_en\"]['label'].replace({0:1, 1:1, 2:0}, inplace=True)\n",
   "id": "9386ecdbb4035374",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'implicit_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimplicit_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace({\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m:\u001B[38;5;241m0\u001B[39m}, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'implicit_en'"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:28.353303Z",
     "start_time": "2025-01-20T18:42:28.333362Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict[\"implicit_en\"].label.value_counts()",
   "id": "b36b60dc81b409a6",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'implicit_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[80], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimplicit_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mvalue_counts()\n",
      "\u001B[0;31mKeyError\u001B[0m: 'implicit_en'"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:28.849134Z",
     "start_time": "2025-01-20T18:42:28.820464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dynabench 2021 / English\n",
    "df_dict[\"dyn21_en\"].label.replace({\"hate\":1, \"nothate\":0}, inplace=True)\n",
    "\n",
    "# Founta 2018 / English\n",
    "df_dict[\"fou18_en\"].label.replace({'hateful': 1, \"abusive\": 0, \"normal\": 0, \"spam\": 0}, inplace = True)\n",
    "\n",
    "# Kennedy 2020 / English\n",
    "df_dict[\"ken20_en\"].rename(columns={\"label_hate_maj\": \"label\"}, inplace=True)\n",
    "\n",
    "# Fortuna 2019 / Portuguese\n",
    "df_dict[\"for19_pt\"].rename(columns={\"hatespeech_comb\": \"label\"}, inplace=True)\n",
    "\n",
    "# Basile 2019 / Spanish\n",
    "df_dict[\"bas19_es\"].rename(columns={\"HS\": \"label\"}, inplace=True)\n",
    "\n",
    "# Sanguinetti 2020 / Italian\n",
    "df_dict[\"san20_it\"].rename(columns={\"hs\": \"label\"}, inplace=True)\n",
    "\n",
    "# Ousidhoum 2019 / Arabic & French\n",
    "for d in [\"ous19_ar\", \"ous19_fr\"]:\n",
    "    df_dict[d][\"label\"] = df_dict[d].sentiment.apply(lambda x: 1 if \"hateful\" in x else 0)\n",
    "    # text was already cleaned in a way that conflicts with our later cleaning, so we align it here\n",
    "    df_dict[d][\"text\"] = df_dict[d].tweet.apply(lambda x: x.replace(\"@url\", \"http\"))\n",
    "\n",
    "# HASOC 19, 20 and 21 / Hindi\n",
    "for d in [\"has19_hi\", \"has20_hi\", \"has21_hi\"]:\n",
    "    df_dict[d][\"label\"] = df_dict[d].task_2.apply(lambda x: 1 if x==\"HATE\" else 0)"
   ],
   "id": "7c0a4725edb915dd",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dyn21_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Dynabench 2021 / English\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdyn21_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mreplace({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhate\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnothate\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;241m0\u001B[39m}, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Founta 2018 / English\u001B[39;00m\n\u001B[1;32m      5\u001B[0m df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mreplace({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhateful\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabusive\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnormal\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspam\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m}, inplace \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'dyn21_en'"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:29.043223Z",
     "start_time": "2025-01-20T18:42:29.037336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# drop redundant columns\n",
    "for dataset in df_dict:\n",
    "    if \"split\" in df_dict[dataset].columns:\n",
    "        df_dict[dataset] = df_dict[dataset][[\"text\", \"label\", \"split\"]]\n",
    "    else:\n",
    "        df_dict[dataset] = df_dict[dataset][[\"text\", \"label\"]]"
   ],
   "id": "f13277bd-80b3-4664-854c-9067973ff9c5",
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "id": "3bb486c0-6d69-47de-ab40-1395cff48b78",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5f2180b-49b9-4337-8bf6-918ae0fff373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:30.315760Z",
     "start_time": "2025-01-20T18:42:30.246392Z"
    }
   },
   "source": [
    "def clean_text(text):\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'@user',text) # format expected by XLM-T\n",
    "    text = re.sub(r\"http\\S+\",'http',text) # format expected by XLM-T\n",
    "    text = re.sub(r\"\\n\",' ',text)\n",
    "    text = re.sub(r\"\\t\",' ',text)\n",
    "    text = text.replace(\"[URL]\", \"http\") # format expected by XLM-T\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "for dataset in df_dict:\n",
    "    df_dict[dataset][\"text\"] = df_dict[dataset].text.apply(lambda x: clean_text(x))"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "41906fb5-a64e-4a90-95ab-ca685dec29ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:31.246247Z",
     "start_time": "2025-01-20T18:42:31.222473Z"
    }
   },
   "source": [
    "# boost Kennedy 2020 / English to have 50% hate (up from ca. 30%)\n",
    "df_dict[\"ken20_en\"] = pd.concat([df_dict[\"ken20_en\"][df_dict[\"ken20_en\"].label==1], df_dict[\"ken20_en\"][df_dict[\"ken20_en\"].label==0].sample(11596, random_state=123)]).sample(frac=1, random_state=123)\n",
    "\n",
    "# boost Founta 2018 / English to have 22% hate, which is max possible (up from ca. 5%)\n",
    "df_dict[\"fou18_en\"] = pd.concat([df_dict[\"fou18_en\"][df_dict[\"fou18_en\"].label==1], df_dict[\"fou18_en\"][df_dict[\"fou18_en\"].label==0].sample(17600, random_state=123)]).sample(frac=1, random_state=123)\n"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ken20_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# boost Kennedy 2020 / English to have 50% hate (up from ca. 30%)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mken20_en\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mken20_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mken20_en\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m], df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mken20_en\u001B[39m\u001B[38;5;124m\"\u001B[39m][df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mken20_en\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;241m11596\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m)])\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# boost Founta 2018 / English to have 22% hate, which is max possible (up from ca. 5%)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m][df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m], df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m][df_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfou18_en\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;241m17600\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m)])\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'ken20_en'"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:32.265860Z",
     "start_time": "2025-01-20T18:42:32.247098Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict[\"fou18_en\"][\"text\"]",
   "id": "2cf505fc4435e22c",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fou18_en'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfou18_en\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mKeyError\u001B[0m: 'fou18_en'"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:33.205090Z",
     "start_time": "2025-01-20T18:42:33.195165Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict[\"gahd24_de\"]\n",
   "id": "ced3b197be7cf9e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                         text  \\\n",
       "0                                         Ich denke, alles was man tun kann, ist, es zu ignorieren, so Brown.   \n",
       "1                                           Es sind beides unsere Hunde, aber den schwarzen mag ich nicht so.   \n",
       "2                                                                                             Ich liebe Inder   \n",
       "3                                                                  Und dazu kommt auch noch die »Mode« LGBTQ.   \n",
       "4                                                             Jeder weiß, dass Chinesen kleine Schwänze haben   \n",
       "...                                                                                                       ...   \n",
       "10991                                                               Mein bester Freund ist dunkelhäutig. Und?   \n",
       "10992  Das ist eine gescheite Maßnahme, das sollten wir machen, aber das ist nicht das Ende der Fahnenstange.   \n",
       "10993                Ein großer Teil der Belästigung von Schulschule ist auf die Islamisierung zurückzuführen   \n",
       "10994                                                Araber, Juden, Schwarze, eine Lösung für alle #Endlösung   \n",
       "10995                                                Damals war es zu Hause und die Lilien Corona-geschwächt.   \n",
       "\n",
       "       label  split  \n",
       "0          0  train  \n",
       "1          0    dev  \n",
       "2          0  train  \n",
       "3          1    dev  \n",
       "4          1  train  \n",
       "...      ...    ...  \n",
       "10991      0  train  \n",
       "10992      0    dev  \n",
       "10993      1  train  \n",
       "10994      1  train  \n",
       "10995      0  train  \n",
       "\n",
       "[10996 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ich denke, alles was man tun kann, ist, es zu ignorieren, so Brown.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es sind beides unsere Hunde, aber den schwarzen mag ich nicht so.</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ich liebe Inder</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Und dazu kommt auch noch die »Mode« LGBTQ.</td>\n",
       "      <td>1</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeder weiß, dass Chinesen kleine Schwänze haben</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10991</th>\n",
       "      <td>Mein bester Freund ist dunkelhäutig. Und?</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10992</th>\n",
       "      <td>Das ist eine gescheite Maßnahme, das sollten wir machen, aber das ist nicht das Ende der Fahnenstange.</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>Ein großer Teil der Belästigung von Schulschule ist auf die Islamisierung zurückzuführen</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>Araber, Juden, Schwarze, eine Lösung für alle #Endlösung</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>Damals war es zu Hause und die Lilien Corona-geschwächt.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10996 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:35.342057Z",
     "start_time": "2025-01-20T18:42:35.327199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts_list = []\n",
    "texts_set = set()\n",
    "for dataset in df_dict:\n",
    "    for i in df_dict[dataset][\"text\"]:\n",
    "        texts_list.append(i)\n",
    "        if i in texts_set:\n",
    "            print(dataset, i)\n",
    "            continue\n",
    "        texts_set.add(i)\n",
    "\n",
    "len(texts_list), len(texts_set)"
   ],
   "id": "39837583ad3fc0c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gahd24_de Muslimen die Fresse polieren, dass wäre es doch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10996, 10995)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "id": "310f11a2-57a3-4f6e-85d8-1e9964695e8a",
   "metadata": {},
   "source": [
    "## Show descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "id": "acf6ea87-7c69-4ead-88e6-7dd675650ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:36.855444Z",
     "start_time": "2025-01-20T18:42:36.843866Z"
    }
   },
   "source": [
    "def descriptive_stats(df):\n",
    "    n_total = df.shape[0]\n",
    "    n_hate = df.label.sum()\n",
    "    print(\"{} entries, of which {} ({:.2%}) are hateful.\".format(n_total, n_hate, n_hate/n_total))\n",
    "    return df.label.sum()/len(df), len(df)\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset.upper())\n",
    "    descriptive_stats(df_dict[dataset])\n",
    "    display(df_dict[dataset].sample(5))\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAHD24_DE\n",
      "10996 entries, of which 4666 (42.43%) are hateful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                 text  \\\n",
       "8976                                  Darüber hinaus müssen sie für die Kosten (Schäden), die dieses System verursacht, nach wie vor nicht aufkommen.   \n",
       "6489                                                         Return of Monkey Island ist ein solches Spiel, zumindest kann es ein solches Spiel sein.   \n",
       "3800                                                                                       Der linke Geist hat Qualifikation mit Frauenquote ersetzt.   \n",
       "1120  Ältere Frauen oder Frauen in ländlichen Gebieten tragen zudem eine Burka, die den Körper vollständig verschleiert, wenn sie das Haus verlassen.   \n",
       "2733                                                                 Wie geht es bei Pokémon Arceus weiter, wenn man die Strandrettiche gefunden hat?   \n",
       "\n",
       "      label  split  \n",
       "8976      0  train  \n",
       "6489      0    dev  \n",
       "3800      0    dev  \n",
       "1120      0  train  \n",
       "2733      0  train  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8976</th>\n",
       "      <td>Darüber hinaus müssen sie für die Kosten (Schäden), die dieses System verursacht, nach wie vor nicht aufkommen.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>Return of Monkey Island ist ein solches Spiel, zumindest kann es ein solches Spiel sein.</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>Der linke Geist hat Qualifikation mit Frauenquote ersetzt.</td>\n",
       "      <td>0</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>Ältere Frauen oder Frauen in ländlichen Gebieten tragen zudem eine Burka, die den Körper vollständig verschleiert, wenn sie das Haus verlassen.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>Wie geht es bei Pokémon Arceus weiter, wenn man die Strandrettiche gefunden hat?</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:37.602632Z",
     "start_time": "2025-01-20T18:42:37.599073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_dict = {}\n",
    "# new_dict[\"xdomain_tr\"] = df_dict[\"xdomain_tr\"]\n",
    "# new_dict[\"xdomain_en\"] = df_dict[\"xdomain_en\"]\n",
    "new_dict[\"gahd24_de\"] = df_dict[\"gahd24_de\"]\n",
    "# new_dict[\"xplain_en\"] = df_dict[\"xplain_en\"]\n",
    "# new_dict[\"measure_en\"] = df_dict[\"measure_en\"]\n",
    "# df_dict = new_dict"
   ],
   "id": "5ca00f8776d22ae1",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:39.275768Z",
     "start_time": "2025-01-20T18:42:39.271023Z"
    }
   },
   "cell_type": "code",
   "source": "df_dict.keys()",
   "id": "e85c49b770398a7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gahd24_de'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:40.014084Z",
     "start_time": "2025-01-20T18:42:40.010877Z"
    }
   },
   "cell_type": "code",
   "source": "df = new_dict['gahd24_de']",
   "id": "b5a0a7d5b4d7e108",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:42:40.777237Z",
     "start_time": "2025-01-20T18:42:40.769617Z"
    }
   },
   "cell_type": "code",
   "source": "df.split.value_counts()",
   "id": "94f9f2b3596bc39d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7701\n",
       "dev      1649\n",
       "test     1646\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:43:13.946343Z",
     "start_time": "2025-01-20T18:43:13.925600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = df[df['split'] == 'train']\n",
    "dev_df = df[df['split'] == 'dev'] #df[df['split'] == 'validation']\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(train_df.shape, dev_df.shape, test_df.shape)\n",
    "# Adjust dev and test sizes\n",
    "dev_to_test = dev_df.sample(n=2000 - len(test_df), random_state=123)  # Sample 354 from dev to add to test\n",
    "remaining_dev = dev_df.drop(dev_to_test.index)\n",
    "print(remaining_dev.shape, dev_to_test.shape)\n",
    "\n",
    "# Create new test set by combining existing test and sampled dev\n",
    "new_test_df = pd.concat([test_df, dev_to_test])\n",
    "print(remaining_dev.shape, dev_to_test.shape, new_test_df.shape)\n",
    "\n",
    "# Split the remaining dev into dev (500 instances) and extra_dev\n",
    "# if len(remaining_dev) > 500:\n",
    "new_dev_df = remaining_dev.sample(n=500, random_state=123)\n",
    "extra_dev_df = remaining_dev.drop(new_dev_df.index)\n",
    "\n",
    "train_df = train_df.drop(columns=['split'])\n",
    "new_dev_df = new_dev_df.drop(columns=['split'])\n",
    "new_test_df = new_test_df.drop(columns=['split'])\n",
    "extra_dev_df = extra_dev_df.drop(columns=['split'])"
   ],
   "id": "e3f5432cbc1a5ec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7701, 3) (1649, 3) (1646, 3)\n",
      "(1295, 3) (354, 3)\n",
      "(1295, 3) (354, 3) (2000, 3)\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:43:15.144681Z",
     "start_time": "2025-01-20T18:43:15.142031Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b2efb22fe28cb381",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:43:15.465181Z",
     "start_time": "2025-01-20T18:43:15.403804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = '/mounts/data/proj/faeze/data_efficient_hate/datasets/main/1_clean/gahd24_de/'\n",
    "# df_dict['xplain_en'] = train_df\n",
    "# Save the adjusted splits to separate files\n",
    "train_df.to_csv(path+f'train_{train_df.shape[0]}.csv', index=False)\n",
    "new_dev_df.to_csv(path + f'dev_{new_dev_df.shape[0]}.csv', index=False)\n",
    "new_test_df.to_csv(path + f'test_{new_test_df.shape[0]}.csv', index=False)\n",
    "extra_dev_df.to_csv(path + f'extra_dev_{extra_dev_df.shape[0]}.csv', index=False)\n"
   ],
   "id": "a85fb2b984e5fbfa",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:43:01.864886Z",
     "start_time": "2025-01-20T18:43:01.859933Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.shape, new_dev_df.shape, extra_dev_df.shape, new_test_df.shape",
   "id": "f536203069faa9c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7701, 2), (500, 2), (0, 2), (2000, 2))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:39:54.545113Z",
     "start_time": "2025-01-20T15:39:54.542627Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "13d569f37998f5e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99d2e82a-460f-4fe4-b777-fe34ae08c5f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:39:49.537549Z",
     "start_time": "2025-01-20T18:39:49.475288Z"
    }
   },
   "source": [
    "# set aside 2k from each dataset for testing and 500 for dev\n",
    "# except for Ousidhoum in French and Arabic, where train set would otherwise be too small\n",
    "# and for HASOC 20 and 21 in Hindi, where test splits are given\n",
    "\n",
    "TEST_SIZE = 2000\n",
    "DEV_SIZE = 500\n",
    "\n",
    "for dataset in df_dict:\n",
    "    if \"ous19_fr\" in dataset:\n",
    "        df_dict[dataset], devtest = train_test_split(df_dict[dataset], test_size = 2000, random_state=123)\n",
    "        devset, testset = train_test_split(devtest, test_size = 1500, random_state=123)\n",
    "        devset.to_csv(PATH + f\"/1_clean/{dataset}/dev_500.csv\", index=False)\n",
    "        testset.to_csv(PATH + f\"/1_clean/{dataset}/test_1500.csv\", index=False)\n",
    "    elif \"ous19_ar\" in dataset:\n",
    "        df_dict[dataset], devtest = train_test_split(df_dict[dataset], test_size = 1300, random_state=123)\n",
    "        devset, testset = train_test_split(devtest, test_size = 1000, random_state=123)\n",
    "        devset.to_csv(PATH + f\"/1_clean/{dataset}/dev_300.csv\", index=False)\n",
    "        testset.to_csv(PATH + f\"/1_clean/{dataset}/test_1000.csv\", index=False)\n",
    "    elif \"has19_hi\" in dataset or \"has20_hi\" in dataset: # use provided test sets\n",
    "        df_dict[dataset][df_dict[dataset][\"split\"]==\"test\"].to_csv(PATH + f\"/1_clean/{dataset}/test_{len(df_dict[dataset][df_dict[dataset]['split']=='test'])}.csv\", index=False)\n",
    "        df_dict[dataset], devset = train_test_split(df_dict[dataset][df_dict[dataset][\"split\"]==\"train\"], test_size = 500, random_state=123)\n",
    "        devset.to_csv(PATH + f\"/1_clean/{dataset}/dev_500.csv\", index=False)\n",
    "    elif 'xplain_en' not in dataset:\n",
    "        df_dict[dataset], devtest = train_test_split(df_dict[dataset], test_size = TEST_SIZE+DEV_SIZE, random_state=123)\n",
    "        devset, testset = train_test_split(devtest, test_size = TEST_SIZE, random_state=123)\n",
    "        devset.to_csv(PATH + f\"/1_clean/{dataset}/dev_{DEV_SIZE}.csv\", index=False)\n",
    "        testset.to_csv(PATH + f\"/1_clean/{dataset}/test_{TEST_SIZE}.csv\", index=False)\n",
    "        \n",
    "# export all data that is not test or dev, so we can use it for full sample training\n",
    "for dataset in df_dict:\n",
    "    df_dict[dataset].to_csv(PATH + f\"/1_clean/{dataset}/train_{len(df_dict[dataset])}.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "da900890-72e9-4804-8570-6758c9d285bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:39:50.885346Z",
     "start_time": "2025-01-20T18:39:50.412251Z"
    }
   },
   "source": [
    "# create differently-sized train portions from rest of data\n",
    "\n",
    "SEEDS = 10 # for repeated experiments with different random data selection\n",
    "N_RANGE = [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 20000]\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset.upper())\n",
    "    for n in N_RANGE:\n",
    "        \n",
    "        # save all splits for English test sets\n",
    "        if n<len(df_dict[dataset]) and (\"_en\" in dataset):\n",
    "            print(f\"  saving n = {n} training set\")\n",
    "            for random_state in range(1, SEEDS+1):\n",
    "                df_dict[dataset].sample(n, random_state = random_state).to_csv(PATH + f\"/1_clean/{dataset}/train/train_{n}_rs{random_state}.csv\",index=False)\n",
    "        \n",
    "        # save splits up to 2k for other datasets\n",
    "        elif n<len(df_dict[dataset]) and n<=2000: \n",
    "            print(f\"  saving n = {n} training set\")\n",
    "            for random_state in range(1, SEEDS+1):  \n",
    "                df_dict[dataset].sample(n, random_state = random_state).to_csv(PATH + f\"/1_clean/{dataset}/train/train_{n}_rs{random_state}.csv\",index=False)\n",
    "    \n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAHD24_DE\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 30 training set\n",
      "  saving n = 40 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 300 training set\n",
      "  saving n = 400 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "84561adf-666c-48d5-92c6-bce907d20301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:40:04.955881Z",
     "start_time": "2025-01-20T15:40:04.951755Z"
    }
   },
   "source": "df_dict.keys()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gahd24_de'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7396e5cb9ea07993"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
